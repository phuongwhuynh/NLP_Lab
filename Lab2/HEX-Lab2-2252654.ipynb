{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFD7DVZ-xKdT"
   },
   "source": [
    "# Homework Lab 2: Text Preprocessing with Vietnamese\n",
    "**Overview:** In this exercise, we will build a text preprocessing program for Vietnamese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOAeiqdrxKdt"
   },
   "source": [
    "Import the necessary libraries. Note that we are using the underthesea library for Vietnamese tokenization. To install it, follow the instructions below. ([link](https://github.com/undertheseanlp/underthesea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RrFQ_Ht_xKdu"
   },
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import codecs\n",
    "import sys\n",
    "import re\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hC27lBQZxKdw"
   },
   "source": [
    "## Question 1: Create a Corpus and Survey the Data\n",
    "\n",
    "The data in this section is partially extracted from the [VNTC](https://github.com/duyvuleo/VNTC) dataset. VNTC is a Vietnamese news dataset covering various topics. In this section, we will only process the science topic from VNTC. We will create a corpus from both the train and test directories. Complete the following program:\n",
    "\n",
    "- Write `sentences_list` to a file named `dataset_name.txt`, with each element as a document on a separate line.\n",
    "- Check how many documents are in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GyNKT8wAxKdx",
    "outputId": "b2eb7c10-da8d-49cb-8b7d-4f6543700cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels = test labels\n",
      "Total number of documents: 3916\n",
      "The first document: Ninh Thuận: Địa điểm ưu tiên đặt nhà máy điện hạt nhân Một góc Ninh Thuận, địa điểm ưu tiên lựa chọn đặt nhà máy điện hạt nhân Ông Vương Hữu Tấn, Viện trưởng Viện Năng Lượng Nguyên tử Việt Nam cho biết, có ba địa điểm hiện đang được cân nhắc để lựa chọn làm nơi đặt nhà máy điện hạt nhân.  Đó là Ninh Thuận, Phú Yên và Phan Rang. Tuy nhiên, Ninh Thuận vẫn là địa điểm được ưu tiên lựa chọn.  Được hỏi về tiến triển của dự án xây dựng nhà máy điện hạt nhân tại VN, vẫn theo ông Vương Hữu Tấn, Viện Năng lượng Nguyên tử Việt Nam đã hoàn tất dự án tiền khả thi và đang trình Chính phủ phê duyệt.  Dự kiến, đến năm tới (2007), sẽ chuyển sang dự án khả thi nhằm đáp ứng kịp tiến độ thời gian xây dựng nhà máy điện hạt nhân tại VN.  Theo kế hoạch, nhà máy điện hạt nhân đầu tiên sẽ được xây dựng và đi vào hoạt động bắt đầu từ năm 2020 với quy mô công suất từ 2.000 MW - 4.000 MW, và sẽ chiếm từ 5-9% tổng công suất phát điện toàn quốc.  Về vấn đề lựa chọn công nghệ nào cho nhà máy điện hạt nhân ở VN, Viện Năng lượng Nguyên tử Việt Nam cho biết, hiện vẫn đang còn trong quá trình phân tích và trưng cầu ý kiến của các chuyên gia.  Tuy nhiên, công nghệ của nhà máy điện hạt nhân sẽ được chọn lựa trên cơ sở đảm bảo an toàn cao nhất trong quá trình vận hành.  Theo hướng dẫn của Cơ quan Năng lượng nguyên tử quốc tế (IAEA), để đưa một nhà máy điện hạt nhân vào hoạt động thì cần khoảng 3.500- 4.500 người, trong đó có khoảng 500-700 người có trình độ đại học và trên đại học, 700-1.000 kỹ thuật viên và 2.200-3.000 công nhân lành nghề các loại Hiện nay, Viện có 681 cán bộ với tuổi trung bình là 42, trong đó đại học 361 người, thạc sỹ 78 người, tiến sỹ và GS, PGS 62 người. \"Với tuổi đời trung bình của cán bộ Viện Năng lượng Nguyên tử Việt Nam là 42 thì đây là độ tuổi so với VN là trẻ, nhưng so với quốc tế là… già!\" -đại diện Bộ Khoa học-Công nghệ nhận xét .\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"VNTC_khoahoc\"\n",
    "path = ['./VNTC_khoahoc/Train_Full/', './VNTC_khoahoc/Test_Full/']\n",
    "\n",
    "if os.listdir(path[0]) == os.listdir(path[1]):\n",
    "    folder_list = [os.listdir(path[0]), os.listdir(path[1])]\n",
    "    print(\"train labels = test labels\")\n",
    "else:\n",
    "    print(\"train labels differ from test labels\")\n",
    "\n",
    "doc_num = 0\n",
    "sentences_list = []\n",
    "meta_data_list = []\n",
    "for i in range(2):\n",
    "    for folder_name in folder_list[i]:\n",
    "        folder_path = path[i] + folder_name\n",
    "        if folder_name[0] != \".\":\n",
    "            for file_name in glob.glob(os.path.join(folder_path, '*.txt')):\n",
    "                # Read the file content into f\n",
    "                f = codecs.open(file_name, 'br')\n",
    "                # Convert the data to UTF-16 format for Vietnamese text\n",
    "                file_content = (f.read().decode(\"utf-16\")).replace(\"\\r\\n\", \" \")\n",
    "                sentences_list.append(file_content.strip())\n",
    "                f.close\n",
    "                # Count the number of documents\n",
    "                doc_num += 1\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "output_file=f\"{dataset_name}.txt\"\n",
    "file=open(output_file, \"w\", encoding=\"utf-8\")\n",
    "for sentence in sentences_list:\n",
    "    file.write(sentence + '\\n')\n",
    "file.close()\n",
    "print(f\"Total number of documents: {doc_num}\")\n",
    "print(f\"The first document: {sentences_list[0]}\")\n",
    "#### END YOUR CODE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Write Preprocessing Functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KXHcDpuxKd0"
   },
   "source": [
    "### Question 2.1: Write a Function to Clean Text\n",
    "Hint:\n",
    "- The text should only retain the following characters: aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\'\\\n",
    "- Then trim the whitespace in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "k8hIglDXxKd0"
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    allowed_chars = \"aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\\\'\"\n",
    "    cleaned = re.sub(f\"[^{allowed_chars}]\", \" \", string)\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KfXstqAxKd1"
   },
   "source": [
    "### Question 2.2: Write a Function to Convert Text to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KRwgVjxhxKd1"
   },
   "outputs": [],
   "source": [
    "# make all text lowercase\n",
    "def text_lowercase(string):\n",
    "    return string.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYM_GO_5xKd2"
   },
   "source": [
    "### Question 2.3: Tokenize Words\n",
    "Hint: Use the `word_tokenize()` function imported above with two parameters: `strings` and `format=\"text\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "pty34NwyxKd2"
   },
   "outputs": [],
   "source": [
    "def tokenize(strings):\n",
    "    return word_tokenize(strings, format=\"text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gQGmL4gxKd2"
   },
   "source": [
    "### Question 2.4: Remove Stop Words\n",
    "To remove stop words, we use a list of Vietnamese stop words stored in the file `./vietnamese-stopwords.txt`. Complete the following program:\n",
    "- Check each word in the text (`strings`). If a word is not in the stop words list, add it to `doc_words`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "aqStv2rPxKd3"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(strings):\n",
    "    stopwords_path = \"vietnamese-stopwords.txt\"\n",
    "    with open(stopwords_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        stop_words = set(f.read().splitlines().replace(\" \", \"_\"))\n",
    "    words = strings.split()\n",
    "    doc_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(doc_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUNOKigIxKd4"
   },
   "source": [
    "## Question 2.5: Build a Preprocessing Function\n",
    "Hint: Call the functions `clean_str`, `text_lowercase`, `tokenize`, and `remove_stopwords` in order, then return the result from the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_vd-el91xKd_"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(strings):\n",
    "    #### YOUR CODE HERE ####\n",
    "    strings=clean_str(strings)\n",
    "    strings=text_lowercase(strings)\n",
    "    strings=tokenize(strings)\n",
    "    strings=remove_stopwords(strings)\n",
    "    return strings\n",
    "    #### END YOUR CODE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BGOqa1mxKeA"
   },
   "source": [
    "## Question 3: Perform Preprocessing\n",
    "Now, we will read the corpus from the file created in Question 1. After that, we will call the preprocessing function for each document in the corpus.\n",
    "\n",
    "Hint: Call the `text_preprocessing()` function with `doc_content` as the input parameter and save the result in the variable `temp1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "length of clean_docs =  3916\n",
      "clean_docs[0]:\n",
      "ninh thuận_địa_điểm ưu_tiên nhà_máy điện hạt_nhân góc ninh_thuận , địa_điểm ưu_tiên lựa_chọn nhà_máy điện hạt_nhân vương_hữu , viện trưởng viện năng_lượng nguyên_tử việt_nam , địa_điểm hiện cân_nhắc lựa_chọn nhà_máy điện hạt_nhân ninh_thuận , phú_yên phan_rang tuy_nhiên , ninh_thuận địa_điểm ưu_tiên lựa_chọn tiến_triển dự_án xây_dựng nhà_máy điện hạt_nhân vn , vương_hữu , viện năng_lượng nguyên_tử việt_nam hoàn_tất dự_án tiền_khả_thi trình chính_phủ phê_duyệt dự_kiến , ( 2007 ) , dự_án khả_thi đáp_ứng kịp tiến_độ thời_gian xây_dựng nhà_máy điện hạt_nhân vn kế_hoạch , nhà_máy điện hạt_nhân đầu_tiên xây_dựng đi hoạt_động bắt_đầu 2020 quy_mô công_suất 2 000 mw 4 000 mw , chiếm 5 9 tổng_công_suất phát_điện toàn_quốc vấn_đề lựa_chọn công_nghệ nhà_máy điện hạt_nhân vn , viện năng_lượng nguyên_tử việt_nam , hiện quá_trình phân_tích trưng_cầu ý_kiến chuyên_gia tuy_nhiên , công_nghệ nhà_máy điện hạt_nhân chọn_lựa cơ_sở đảm_bảo an_toàn quá_trình vận_hành hướng_dẫn cơ_quan năng_lượng nguyên_tử quốc_tế ( iaea ) , nhà_máy điện hạt_nhân hoạt_động 3 500 4 500 , 500 700 trình_độ đại_học đại_học , 700 1 000 kỹ_thuật_viên 2 200 3 000 công_nhân lành_nghề hiện_nay , viện 681 cán_bộ trung_bình 42 , đại_học 361 , thạc_sỹ 78 , tiến_sỹ gs , pgs 62 tuổi_đời trung_bình cán_bộ viện năng_lượng nguyên_tử việt_nam 42 độ vn trẻ , quốc_tế già ! đại_diện khoa_học công_nghệ nhận_xét\n"
     ]
    }
   ],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "clean_docs = []\n",
    "#### END YOUR CODE #####\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        clean_docs.append(text_preprocessing(line.strip()))\n",
    "\n",
    "print(\"\\nlength of clean_docs = \", len(clean_docs))\n",
    "print('clean_docs[0]:\\n' + clean_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFhai6BwxKeB"
   },
   "source": [
    "## Question 4: Save Preprocessed Data\n",
    "Hint: Save the preprocessed data to a file named `dataset_name + '.clean.txt'`, where each document is written on a separate line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xfHmSiRrxKeB"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "clean_output_file = f\"{dataset_name}.clean.txt\"\n",
    "with open(clean_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for doc in clean_docs:\n",
    "        f.write(doc + \"\\n\")\n",
    "\n",
    "#### YOUR CODE HERE ####"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
