{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home exercise below\n",
    "a) Build a language model based on n-grams using the Laplace smoothing method for the following models:\n",
    "- 1-gram\n",
    "- 2-gram\n",
    "- 3-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries: nltk, gdown,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import math\n",
    "import gdown\n",
    "nltk.download('punkt')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gdown to download tedtalk.txt from the google drive link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?export=download&id=1tQ9zW0ihL3Uc6GIge9AIV1pQnHYI7ihI\n",
      "To: c:\\HCMUT\\Study\\HK242\\NLP\\Exercise\\Lab3\\Home Exercise\\tedtalk.txt\n",
      "100%|██████████| 40.3M/40.3M [00:01<00:00, 37.6MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tedtalk.txt'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=f\"https://drive.google.com/uc?export=download&id=1tQ9zW0ihL3Uc6GIge9AIV1pQnHYI7ihI\"\n",
    "output_path=\"tedtalk.txt\"\n",
    "gdown.download(url,output_path, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the plain text file and read into `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text corpus from file\n",
    "with open('tedtalk.txt', 'r', encoding='utf-8') as file:\n",
    "    corpus = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the text by lowering, and keeping only `[a-z0-9 .!?\\']`.\n",
    "Then use `nltk.sent_tokenize()` to tokenize text into list of sentences and tokenize each sentence into list of words using `nltk.tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text: lowercase, remove punctuation, tokenize\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    cleaned_text = re.sub(r\"[^a-z0-9 .!?\\']\", \" \", text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def split_sentences_and_tokenize(text):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    return tokenized_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate ngrams from sentences. Add `<s>` and `</s>` for 2-gram and 3-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n-grams\n",
    "def generate_ngrams_from_sentences(sentences, n):\n",
    "    ngrams_list = []\n",
    "    for sentence in sentences:\n",
    "        padded_sentence=['<s>']+sentence #To efficiently calculate prefix\n",
    "        if n>1:\n",
    "            padded_sentence = ['<s>'] * (n - 1) + padded_sentence + ['</s>'] \n",
    "        ngrams_list.extend(list(ngrams(padded_sentence, n)))\n",
    "    return ngrams_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to precompute ngram probablity to optimize runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute n-gram probabilities with Laplace smoothing\n",
    "def compute_ngram_probs(vocabulary_size,token_size, n,ngram_counts, alpha=1):\n",
    "    probs = {}\n",
    "    \n",
    "    for ngram, count in ngram_counts[n-1].items():\n",
    "        if n == 1:  # For unigrams, no need for prefix_count\n",
    "            prefix_count = token_size \n",
    "        else:  # For bigrams or trigrams, use the prefix\n",
    "            prefix = ngram[:-1]  \n",
    "            prefix_count = ngram_counts[n-2].get(prefix, 0)\n",
    "        # Apply Laplace smoothing\n",
    "        probs[ngram] = (count + alpha) / (prefix_count + alpha * vocabulary_size)\n",
    "    \n",
    "    return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess and tokenize based on sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['thank', 'you', 'so', 'much', 'chris', '.'], ['and', 'it', \"'s\", 'truly', 'a', 'great', 'honor', 'to', 'have', 'the', 'opportunity', 'to', 'come', 'to', 'this', 'stage', 'twice', 'i', \"'m\", 'extremely', 'grateful', '.'], ['i', 'have', 'been', 'blown', 'away', 'by', 'this', 'conference', 'and', 'i', 'want', 'to', 'thank', 'all', 'of', 'you', 'for', 'the', 'many', 'nice', 'comments', 'about', 'what', 'i', 'had', 'to', 'say', 'the', 'other', 'night', '.'], ['and', 'i', 'say', 'that', 'sincerely', 'partly', 'because', 'mock', 'sob', 'i', 'need', 'that', '.'], ['laughter', 'put', 'yourselves', 'in', 'my', 'position', '.'], ['laughter', 'i', 'flew', 'on', 'air', 'force', 'two', 'for', 'eight', 'years', '.'], ['laughter', 'now', 'i', 'have', 'to', 'take', 'off', 'my', 'shoes', 'or', 'boots', 'to', 'get', 'on', 'an', 'airplane', '!'], ['laughter', 'applause', 'i', \"'ll\", 'tell', 'you', 'one', 'quick', 'story', 'to', 'illustrate', 'what', 'that', \"'s\", 'been', 'like', 'for', 'me', '.'], ['laughter', 'it', \"'s\", 'a', 'true', 'story', 'every', 'bit', 'of', 'this', 'is', 'true', '.'], ['soon', 'after', 'tipper', 'and', 'i', 'left', 'the', 'mock', 'sob', 'white', 'house', 'laughter', 'we', 'were', 'driving', 'from', 'our', 'home', 'in', 'nashville', 'to', 'a', 'little', 'farm', 'we', 'have', '50', 'miles', 'east', 'of', 'nashville', '.'], ['driving', 'ourselves', '.'], ['laughter', 'i', 'know', 'it', 'sounds', 'like', 'a', 'little', 'thing', 'to', 'you', 'but', 'laughter', 'i', 'looked', 'in', 'the', 'rear', 'view', 'mirror', 'and', 'all', 'of', 'a', 'sudden', 'it', 'just', 'hit', 'me', '.'], ['there', 'was', 'no', 'motorcade', 'back', 'there', '.'], ['laughter', 'you', \"'ve\", 'heard', 'of', 'phantom', 'limb', 'pain', '?'], ['laughter', 'this', 'was', 'a', 'rented', 'ford', 'taurus', '.'], ['laughter', 'it', 'was', 'dinnertime', 'and', 'we', 'started', 'looking', 'for', 'a', 'place', 'to', 'eat', '.'], ['we', 'were', 'on', 'i', '40.', 'we', 'got', 'to', 'exit', '238', 'lebanon', 'tennessee', '.'], ['we', 'got', 'off', 'the', 'exit', 'we', 'found', 'a', 'shoney', \"'s\", 'restaurant', '.'], ['low', 'cost', 'family', 'restaurant', 'chain', 'for', 'those', 'of', 'you', 'who', 'do', \"n't\", 'know', 'it', '.'], ['we', 'went', 'in', 'and', 'sat', 'down', 'at', 'the', 'booth', 'and', 'the', 'waitress', 'came', 'over', 'made', 'a', 'big', 'commotion', 'over', 'tipper', '.']]\n"
     ]
    }
   ],
   "source": [
    "sentences=preprocess(corpus)\n",
    "sentences = split_sentences_and_tokenize(sentences)\n",
    "print(sentences[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate total number of tokens, calculate the vocabulary size, count unigram, bigram, trigram and use them to precompute the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens:  7835609\n",
      "Vocabulary size:  69944\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [token for sentence in sentences for token in sentence]\n",
    "vocabulary_size = len(set(all_tokens))  \n",
    "token_size=len(all_tokens)\n",
    "print(\"Total tokens: \",token_size)\n",
    "print(\"Vocabulary size: \", vocabulary_size)\n",
    "ngram_counts=[]\n",
    "ngram_probs=[]\n",
    "for x in range(3):\n",
    "    ngram_counts.append(Counter(generate_ngrams_from_sentences(sentences,x+1)))\n",
    "for x in range(3):\n",
    "    ngram_probs.append(compute_ngram_probs(vocabulary_size,token_size,x+1,ngram_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Unigram Count: [(('<s>',), 441023), (('thank',), 5195), (('you',), 108403), (('so',), 57089), (('much',), 9220), (('chris',), 498), (('.',), 400768), (('and',), 240579), (('it',), 115807), ((\"'s\",), 85344), (('truly',), 695), (('a',), 170846), (('great',), 4756), (('honor',), 277), (('to',), 210927), (('have',), 44958), (('the',), 336385), (('opportunity',), 1156), (('come',), 6146), (('this',), 74906)]\n",
      "Sample Bigram Count: [(('<s>', '<s>'), 441023), (('<s>', 'thank'), 3222), (('thank', 'you'), 5001), (('you', 'so'), 356), (('so', 'much'), 1751), (('much', 'chris'), 4), (('chris', '.'), 56), (('.', '</s>'), 400542), (('<s>', 'and'), 60110), (('and', 'it'), 8960), (('it', \"'s\"), 32904), ((\"'s\", 'truly'), 25), (('truly', 'a'), 20), (('a', 'great'), 1219), (('great', 'honor'), 17)]\n",
      "Sample Trigram Count: [(('<s>', '<s>', '<s>'), 441023), (('<s>', '<s>', 'thank'), 3222), (('<s>', 'thank', 'you'), 3194), (('thank', 'you', 'so'), 259), (('you', 'so', 'much'), 261), (('so', 'much', 'chris'), 3), (('much', 'chris', '.'), 4), (('chris', '.', '</s>'), 56), (('<s>', '<s>', 'and'), 60110), (('<s>', 'and', 'it'), 3260), (('and', 'it', \"'s\"), 3590), (('it', \"'s\", 'truly'), 14), ((\"'s\", 'truly', 'a'), 4), (('truly', 'a', 'great'), 1), (('a', 'great', 'honor'), 9)]\n",
      "Sample Unigram Probabilities: [(('<s>',), 0.055786609741279324), (('thank',), 0.0006572595237803099), (('you',), 0.013712386723610607), (('so',), 0.007221506199503058), (('much',), 0.0011663953173168277), (('chris',), 6.312018906204284e-05), (('.',), 0.05069461933908988), (('and',), 0.030431773716525588), (('it',), 0.014648943596988092), ((\"'s\",), 0.010795576223446988), (('truly',), 8.803938193824012e-05), (('a',), 0.02161101190517602), (('great',), 0.0006017289366094947), (('honor',), 3.516515542935453e-05), (('to',), 0.02668099246188091)]\n",
      "Sample Bigram Probabilities: [(('<s>', '<s>'), 0.863116404777608), (('<s>', 'thank'), 0.006307648047721282), (('thank', 'you'), 0.06656995701300257), (('you', 'so'), 0.0020017157563625965), (('so', 'much'), 0.0137916919225713), (('much', 'chris'), 6.316002223232782e-05), (('chris', '.'), 0.0008091763436586127), (('.', '</s>'), 0.8509300803888578), (('<s>', 'and'), 0.1176416480907769), (('and', 'it'), 0.0288577657693633), (('it', \"'s\"), 0.1771457488788755), ((\"'s\", 'truly'), 0.00016743083818453454), (('truly', 'a'), 0.0002972862016732966), (('a', 'great'), 0.005066655592009635), (('great', 'honor'), 0.00024096385542168674)]\n",
      "Sample Trigram Probabilities: [(('<s>', '<s>', '<s>'), 0.863116404777608), (('<s>', '<s>', 'thank'), 0.006307648047721282), (('<s>', 'thank', 'you'), 0.04366782385260914), (('thank', 'you', 'so'), 0.003469210754553339), (('you', 'so', 'much'), 0.0037268847795163583), (('so', 'much', 'chris'), 5.579189622707302e-05), (('much', 'chris', '.'), 7.148167209927375e-05), (('chris', '.', '</s>'), 0.0008142857142857143), (('<s>', '<s>', 'and'), 0.1176416480907769), (('<s>', 'and', 'it'), 0.025074199947714027), (('and', 'it', \"'s\"), 0.04551100070972321), (('it', \"'s\", 'truly'), 0.0001458462974486621), ((\"'s\", 'truly', 'a'), 7.146021809658563e-05), (('truly', 'a', 'great'), 2.8586130009719286e-05), (('a', 'great', 'honor'), 0.00014052246251563313)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Sample Unigram Probabilities:\", list(ngram_probs[0].items())[1:15])\n",
    "print(\"Sample Bigram Probabilities:\", list(ngram_probs[1].items())[1:15])\n",
    "print(\"Sample Trigram Probabilities:\", list(ngram_probs[2].items())[1:15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate probability of a sentence, fall back to computing again using ngram counts if the precomputation is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_probability(sentence, ngram_probs,ngram_count,token_size,vocabulary_size, n, alpha=1):\n",
    "    tokens = [word for sentence in split_sentences_and_tokenize(preprocess(sentence)) for word in sentence]\n",
    "    prob = 1.0\n",
    "\n",
    "    if n > 1:\n",
    "        tokens = ['<s>'] * (n - 1) + tokens + ['</s>']  # Add boundary tokens for bigram/trigram\n",
    "\n",
    "    # Calculate the probability using precomputed smoothed n-gram probabilities\n",
    "    for i in range(n - 1, len(tokens)):  \n",
    "        ngram = tuple(tokens[i - n + 1:i + 1])  \n",
    "        prob_word = ngram_probs[n-1].get(ngram, 0)  \n",
    "        # Apply Laplace smoothing if the n-gram is unseen\n",
    "        if prob_word == 0:  \n",
    "            if n!=1:\n",
    "                prefix = ngram[:-1]  \n",
    "                prefix_count = ngram_count[n-2].get(prefix, 0)  \n",
    "            else:\n",
    "                prefix_count=token_size\n",
    "            prob_word = (alpha) / (prefix_count + alpha * vocabulary_size)\n",
    "        prob *= prob_word\n",
    "\n",
    "    return prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate perplexity of a sentence, fall back to computing again using ngram counts if the precomputation is not found. The perplexity is calculated using logarith instead of inverse to avoid underflowing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(sentence, ngram_probs,ngram_count,token_size,vocabulary_size, n, alpha=1):\n",
    "    tokens = [word for sentence in split_sentences_and_tokenize(preprocess(sentence)) for word in sentence]\n",
    "    offset=len(tokens)+n-1\n",
    "    if n > 1:\n",
    "        tokens = ['<s>'] * (n - 1) + tokens + ['</s>']  # Add boundary tokens for bigram/trigram\n",
    "    total_log_prob = 0.0\n",
    "    for i in range(n - 1, len(tokens)):  \n",
    "        ngram = tuple(tokens[i - n + 1:i + 1])  \n",
    "        prob_word = ngram_probs[n-1].get(ngram, 0)  \n",
    "        \n",
    "        # Apply Laplace smoothing if the n-gram is unseen\n",
    "        if prob_word == 0:\n",
    "            if n!=1:\n",
    "                prefix = ngram[:-1]  # Prefix is the first (n-1) tokens\n",
    "                prefix_count = ngram_count[n-2].get(prefix, 0)  # Get prefix count (smoothed)\n",
    "            else:\n",
    "                prefix_count=token_size\n",
    "            prob_word = (alpha) / (prefix_count + alpha * vocabulary_size)\n",
    "        # Perplexity calculation using logarith instead of inverse\n",
    "        total_log_prob += math.log(prob_word) if prob_word > 0 else float('-inf')\n",
    "\n",
    "    perplexity = math.exp(-total_log_prob / (offset)) if total_log_prob > float('-inf') else float('inf')\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Calculate the probability of a sentence and compute the Perplexity of a sentence based on 1-gram, 2-gram, and 3-gram models.\n",
    "\n",
    "The sentence chosen is \"I want to speak at ted talk.\" This will be the base sentence to compare below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Probability: 1.5121885066224436e-21\n",
      "Bigram Probability: 6.671326354226927e-19\n",
      "Trigram Probability: 4.339151870421607e-27\n",
      "Unigram Perplexity: 400.45088554805614\n",
      "Bigram Perplexity: 104.6000709925499\n",
      "Trigram Perplexity: 432.77235916126153\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I want to speak at ted talk.\"\n",
    "unigram_prob = calculate_sentence_probability(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 1)\n",
    "bigram_prob = calculate_sentence_probability(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 2)\n",
    "trigram_prob = calculate_sentence_probability(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 3)\n",
    "\n",
    "print(\"Unigram Probability:\", unigram_prob)\n",
    "print(\"Bigram Probability:\", bigram_prob)\n",
    "print(\"Trigram Probability:\", trigram_prob)\n",
    "\n",
    "print(\"Unigram Perplexity:\", calculate_perplexity(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 1))\n",
    "print(\"Bigram Perplexity:\", calculate_perplexity(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 2))\n",
    "print(\"Trigram Perplexity:\", calculate_perplexity(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Analyze the results (Provide your own examples of spelling errors and calculate the probability of two similar sentences, where one has the correct word order and the other has an incorrect word order).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of spelling errors of the previous sentence, the perplexity is much higher indicating this sentence does not fit with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Probability: 9.250038516821093e-26\n",
      "Bigram Probability: 7.7910449546226906e-31\n",
      "Trigram Probability: 2.3158151749785303e-36\n",
      "Unigram Perplexity: 1346.5796822248376\n",
      "Bigram Perplexity: 2215.0230728151582\n",
      "Trigram Perplexity: 3660.408965940057\n"
     ]
    }
   ],
   "source": [
    "#Example of spelling errors\n",
    "sentence = \"I went too speak ate ted take.\"\n",
    "unigram_prob = calculate_sentence_probability(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 1)\n",
    "bigram_prob = calculate_sentence_probability(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 2)\n",
    "trigram_prob = calculate_sentence_probability(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 3)\n",
    "\n",
    "print(\"Unigram Probability:\", unigram_prob)\n",
    "print(\"Bigram Probability:\", bigram_prob)\n",
    "print(\"Trigram Probability:\", trigram_prob)\n",
    "\n",
    "print(\"Unigram Perplexity:\", calculate_perplexity(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 1))\n",
    "print(\"Bigram Perplexity:\", calculate_perplexity(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 2))\n",
    "print(\"Trigram Perplexity:\", calculate_perplexity(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of wrong order of the above sentence, the perplexity is higher on all 2gram and 3gram models compared with the base sentence but is not as high as the spelling error sentence. This sentence is worse than the base sentence but is better than the spelling error one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Probability: 1.5121885066224436e-21\n",
      "Bigram Probability: 1.15249704944147e-23\n",
      "Trigram Probability: 1.772522028170151e-35\n",
      "Unigram Perplexity: 400.45088554805614\n",
      "Bigram Perplexity: 353.75833926315215\n",
      "Trigram Perplexity: 2986.3508955363095\n"
     ]
    }
   ],
   "source": [
    "#Example of spelling errors\n",
    "sentence = \"I speak at want to ted talk.\"\n",
    "unigram_prob = calculate_sentence_probability(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 1)\n",
    "bigram_prob = calculate_sentence_probability(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 2)\n",
    "trigram_prob = calculate_sentence_probability(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 3)\n",
    "\n",
    "print(\"Unigram Probability:\", unigram_prob)\n",
    "print(\"Bigram Probability:\", bigram_prob)\n",
    "print(\"Trigram Probability:\", trigram_prob)\n",
    "\n",
    "print(\"Unigram Perplexity:\", calculate_perplexity(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 1))\n",
    "print(\"Bigram Perplexity:\", calculate_perplexity(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 2))\n",
    "print(\"Trigram Perplexity:\", calculate_perplexity(sentence, ngram_probs, ngram_counts,token_size, vocabulary_size, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Exercise\n",
    "a) Improve the model by using interpolation smoothing with the \"Stupid Backoff\" method (Brants et al., 2007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_backoff_ngram_probs(vocabulary_size,token_size, n, ngram_counts, alpha=0.4):\n",
    "    probs = {}\n",
    "    \n",
    "    for ngram, count in ngram_counts[n-1].items():\n",
    "        backoff_prob = count\n",
    "        current_ngram = ngram\n",
    "        current_n = n\n",
    "        \n",
    "        while current_n >= 1:\n",
    "            prefix = current_ngram[:-1]  # Drop the first token to back off\n",
    "            if current_n>1:\n",
    "                prefix_count = ngram_counts[current_n-2].get(prefix, 0)\n",
    "                if (prefix_count<backoff_prob):\n",
    "                    print(prefix, \" \",current_ngram)\n",
    "            else:\n",
    "                prefix_count=token_size\n",
    "            \n",
    "            if backoff_prob > 0:\n",
    "                backoff_prob /= prefix_count if prefix_count > 0 else 1  \n",
    "                break \n",
    "            else:\n",
    "                backoff_prob = alpha*prefix_count  # Apply Stupid Backoff\n",
    "                current_ngram = prefix\n",
    "                current_n -= 1\n",
    "        \n",
    "        if current_n == 0 and backoff_prob == 0:  # If reached unigram level with no valid probability\n",
    "            backoff_prob = alpha / vocabulary_size\n",
    "        \n",
    "        probs[ngram] = backoff_prob\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "def calculate_backoff_sentence_probability(sentence, ngram_probs, ngram_count, vocabulary_size, n, beta=0.4):\n",
    "    tokens = [word for sentence in split_sentences_and_tokenize(preprocess(sentence)) for word in sentence]\n",
    "    prob = 1.0\n",
    "\n",
    "    if n > 1:\n",
    "        tokens = ['<s>'] * (n - 1) + tokens + ['</s>']  # Add boundary tokens for bigram/trigram\n",
    "\n",
    "    # Calculate the probability using precomputed n-gram probabilities\n",
    "    for i in range(n - 1, len(tokens)):  \n",
    "        ngram = tuple(tokens[i - n + 1:i + 1])  \n",
    "        prob_word = ngram_probs[n-1].get(ngram, 0)  \n",
    "        currnet_n=n\n",
    "        discount=1\n",
    "        prefix = ngram\n",
    "        while prob_word == 0:  \n",
    "            currnet_n-=1\n",
    "            discount*=beta\n",
    "            prefix = prefix[:-1]  \n",
    "            if currnet_n != 0:\n",
    "                # Back off to the (n-1)-gram\n",
    "                prob_word = ngram_probs[currnet_n-1].get(prefix,0)\n",
    "                prob_word = discount * prob_word\n",
    "            else:\n",
    "                # For unigrams, use the raw frequency count divided by the token size\n",
    "                prob_word = beta  / vocabulary_size\n",
    "                break\n",
    "                \n",
    "        prob *= prob_word\n",
    "\n",
    "    return prob\n",
    "\n",
    "def calculate_backoff_perplexity(sentence, ngram_probs, ngram_count, vocabulary_size, n, beta=0.4):\n",
    "    tokens = [word for sentence in split_sentences_and_tokenize(preprocess(sentence)) for word in sentence]\n",
    "    offset=len(tokens)+n-1\n",
    "    if n > 1:\n",
    "        tokens = ['<s>'] * (n - 1) + tokens + ['</s>']  # Add boundary tokens for bigram/trigram\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "    for i in range(n - 1, len(tokens)):  \n",
    "        ngram = tuple(tokens[i - n + 1:i + 1])  \n",
    "        prob_word = ngram_probs[n-1].get(ngram, 0)  \n",
    "        currnet_n=n\n",
    "        discount=1\n",
    "        prefix = ngram\n",
    "        while prob_word == 0:  \n",
    "            currnet_n-=1\n",
    "            discount*=beta\n",
    "            prefix = prefix[:-1]  \n",
    "            if currnet_n != 0:\n",
    "                # Back off to the (n-1)-gram\n",
    "                prob_word = ngram_probs[currnet_n-1].get(prefix,0)\n",
    "                prob_word = discount * prob_word\n",
    "            else:\n",
    "                # For unigrams, use the raw frequency count divided by the token size\n",
    "                prob_word = beta  / vocabulary_size\n",
    "                break\n",
    "        \n",
    "        # Perplexity calculation using logarithm\n",
    "        total_log_prob += math.log(prob_word) if prob_word > 0 else float('-inf')\n",
    "\n",
    "    perplexity = math.exp(-total_log_prob / (offset)) if total_log_prob > float('-inf') else float('inf')\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Backoff Unigram Probabilities: [(('thank',), 0.0006629988811335532), (('you',), 0.013834661734652661), (('so',), 0.007285840832537713), (('much',), 0.0011766794387009357), (('chris',), 6.355600438970347e-05), (('.',), 0.051147013588860805), (('and',), 0.030703293132671627), (('it',), 0.014779578715579096), ((\"'s\",), 0.01089181453541135), (('truly',), 8.869763664828095e-05), (('a',), 0.021803793425629072), (('great',), 0.0006069726041715456), (('honor',), 3.535143216053787e-05), (('to',), 0.026919030799010008)]\n",
      "Sample Backoff Bigram Probabilities: [(('<s>', 'thank'), 0.007305741423916667), (('thank', 'you'), 0.9626564003849856), (('you', 'so'), 0.0032840419545584532), (('so', 'much'), 0.030671407801853248), (('much', 'chris'), 0.0004338394793926247), (('chris', '.'), 0.11244979919678715), (('.', '</s>'), 0.9994360827211753), (('<s>', 'and'), 0.1362967464281908), (('and', 'it'), 0.03724348342955952), (('it', \"'s\"), 0.28412790245840064), ((\"'s\", 'truly'), 0.0002929321334833146), (('truly', 'a'), 0.02877697841726619), (('a', 'great'), 0.007135080715966426), (('great', 'honor'), 0.0035744322960470985)]\n",
      "Sample Backoff Trigram Probabilities: [(('<s>', '<s>', 'thank'), 0.007305741423916667), (('<s>', 'thank', 'you'), 0.9913097454996896), (('thank', 'you', 'so'), 0.051789642071585686), (('you', 'so', 'much'), 0.7331460674157303), (('so', 'much', 'chris'), 0.0017133066818960593), (('much', 'chris', '.'), 1.0), (('chris', '.', '</s>'), 1.0), (('<s>', '<s>', 'and'), 0.1362967464281908), (('<s>', 'and', 'it'), 0.054233904508401265), (('and', 'it', \"'s\"), 0.40066964285714285), (('it', \"'s\", 'truly'), 0.00042548018477996596), ((\"'s\", 'truly', 'a'), 0.16), (('truly', 'a', 'great'), 0.05), (('a', 'great', 'honor'), 0.007383100902378999)]\n"
     ]
    }
   ],
   "source": [
    "backoff_ngram_probs = []\n",
    "for x in range(3):\n",
    "    backoff_ngram_probs.append(compute_backoff_ngram_probs(vocabulary_size,token_size,x+1,ngram_counts))\n",
    "print(\"Sample Backoff Unigram Probabilities:\", list(backoff_ngram_probs[0].items())[1:15])\n",
    "print(\"Sample Backoff Bigram Probabilities:\", list(backoff_ngram_probs[1].items())[1:15])\n",
    "print(\"Sample Backoff Trigram Probabilities:\", list(backoff_ngram_probs[2].items())[1:15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Compare with the results from In Class Exercise.\n",
    "\n",
    "With backoff calculation, the probability and perplexity is much smaller for both bigram and trigram as it does not need to compensate for the vocabulary size in Laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Probability: 1.6202060302947543e-21\n",
      "Bigram Probability: 2.5918747427400836e-12\n",
      "Trigram Probability: 2.670898297559592e-11\n",
      "Unigram Perplexity: 397.0120782648672\n",
      "Bigram Perplexity: 19.381001767329447\n",
      "Trigram Perplexity: 11.411277452192229\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I want to speak at ted talk.\"\n",
    "unigram_prob = calculate_backoff_sentence_probability(sentence, backoff_ngram_probs, ngram_counts, vocabulary_size, 1)\n",
    "bigram_prob = calculate_backoff_sentence_probability(sentence, backoff_ngram_probs, ngram_counts, vocabulary_size, 2)\n",
    "trigram_prob = calculate_backoff_sentence_probability(sentence, backoff_ngram_probs, ngram_counts, vocabulary_size, 3)\n",
    "\n",
    "print(\"Unigram Probability:\", unigram_prob)\n",
    "print(\"Bigram Probability:\", bigram_prob)\n",
    "print(\"Trigram Probability:\", trigram_prob)\n",
    "\n",
    "print(\"Unigram Perplexity:\", calculate_backoff_perplexity(sentence, backoff_ngram_probs, ngram_counts, vocabulary_size, 1))\n",
    "print(\"Bigram Perplexity:\", calculate_backoff_perplexity(sentence, backoff_ngram_probs, ngram_counts, vocabulary_size, 2))\n",
    "print(\"Trigram Perplexity:\", calculate_backoff_perplexity(sentence, backoff_ngram_probs, ngram_counts, vocabulary_size, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Use the newly built model to generate the next words for a given word sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_word(sequence, backoff_ngram_probs,num_words, max_n=3 ):\n",
    "    # Base case: if num_words is 0, return the current sequence\n",
    "    if num_words == 0:\n",
    "        return sequence\n",
    "\n",
    "    # Start with the highest available n-gram and back off\n",
    "    for n in range(min(max_n, len(backoff_ngram_probs)), 0, -1):\n",
    "        if len(sequence) >= n - 1:\n",
    "            ngram_key = tuple(sequence[-(n-1):]) \n",
    "            ngram_candidates = {}\n",
    "            for ngram, prob in backoff_ngram_probs[n-1].items():\n",
    "                if ngram[:-1] == ngram_key: \n",
    "                    ngram_candidates[ngram[-1]] = prob  \n",
    "            if ngram_candidates:\n",
    "                next_word = max(ngram_candidates, key=ngram_candidates.get)\n",
    "                return generate_next_word(sequence + [next_word], backoff_ngram_probs,num_words - 1,max_n)\n",
    "\n",
    "    # If no match found, fall back to unigram sampling\n",
    "    unigram_probs = {k[0]: v for k, v in backoff_ngram_probs[0].items()}\n",
    "    next_word = max(unigram_probs, key=unigram_probs.get)\n",
    "    return generate_next_word(sequence + [next_word], backoff_ngram_probs, max_n, num_words - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ted', 'talk', 'is', 'about', 'the', 'future', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "current_sequence = [\"ted\",\"talk\",\"is\"]\n",
    "next_word = generate_next_word(current_sequence, backoff_ngram_probs,5)\n",
    "print(next_word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Combine with a function that calculates the distance between words to predict the correct word for a misspelled word position. (from difflib import get_close_matches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "def get_correct_word(word, vocabulary):\n",
    "    # Use difflib to find close matches for a potentially misspelled word\n",
    "    close_matches = get_close_matches(word, vocabulary, n=1, cutoff=0.8)\n",
    "    \n",
    "    if close_matches:\n",
    "        return close_matches[0]\n",
    "    else:\n",
    "        return word  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=set(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ted', 'talk', 'is']\n",
      "['ted', 'talk', 'is', 'about', 'the', 'future', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "current_sequence = [\"ted\",\"tlk\",\"is\"]\n",
    "corrected_sequence = [get_correct_word(word, vocabulary) for word in current_sequence]\n",
    "print(corrected_sequence)\n",
    "next_word = generate_next_word(corrected_sequence, backoff_ngram_probs,5)\n",
    "print(next_word)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
